"""Cralwer uses crawl4ai to download websites as markdown content"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/classes/Crawler.ipynb.

# %% auto 0
__all__ = ['Crawler_ProcessedChunk_Metadata', 'PC_PathNotExist', 'Crawler_ProcessedChunk']

# %% ../../nbs/classes/Crawler.ipynb 2
from dataclasses import dataclass, field
import os
from openai import AsyncOpenAI

import agent_mafia.routes.openai as openai_routes
import agent_mafia.routes.storage as storage_routes
import agent_mafia.prompts.crawler as crawler_prompts

import agent_mafia.client.MafiaError as amme
import agent_mafia.utils.files as amfi

from typing import Union, List
from urllib.parse import urlparse
import datetime as dt

# %% ../../nbs/classes/Crawler.ipynb 3
@dataclass
class Crawler_ProcessedChunk_Metadata:
    source: str
    crawled_at: str
    url_path: str
    chunk_size: int

    @classmethod
    def from_url(cls, source, chunk: str, url):
        return cls(
            source=source,
            crawled_at=dt.datetime.now().isoformat(),
            url_path=urlparse(url).path,
            chunk_size=len(chunk),
        )

    def to_json(self):
        return {
            "source": self.source,
            "crawled_at": self.crawled_at,
            "url_path": self.url_path,
            "chunk_size": self.chunk_size,
        }

# %% ../../nbs/classes/Crawler.ipynb 4
class PC_PathNotExist(amme.MafiaError):
    def __init__(self, md_path):
        super().__init__(f"path {md_path} does not exist")

@dataclass
class Crawler_ProcessedChunk:
    source: str # where a piece of data came from (e.g. a session_id) // could be a complete website or subject area
    url: str
    chunk_number: int
    content: str = field(repr=False) # the actual data 
    title: str = None
    summary: str = None
    embedding: List[float] = None
    metadata: Union[Crawler_ProcessedChunk_Metadata, None] = None
    async_client: Union[AsyncOpenAI, None] = field(repr=False, default=None)
    error_logs: List[str] = field(default_factory=list)

    def __eq__(self, other):
        if self.__class__.__name__ != other.__class__.__name__:
            return False

        return self.url == other.url and self.chunk_number == other.chunk_number

    def __post_init__(self):
        self.metadata = Crawler_ProcessedChunk_Metadata.from_url(
            url=self.url,
            source=self.source,
            chunk=self.content,
        )

    @classmethod
    def from_chunk(
        cls, chunk: str, chunk_number: int, url: str, source: str, output_path=None
    ):

        chunk = cls(
            url=url,
            chunk_number=chunk_number,
            source=source,
            content=chunk,
        )

        if output_path:
            chunk.compare_self_to_disk(output_path)

        return chunk

    @classmethod
    def from_md_file(cls, md_path):
        if not os.path.exists(md_path):
            raise PC_PathNotExist(md_path)

        try:
            chunk, frontmatter = amfi.read_md_from_disk(md_path)

            res = cls(
                url=frontmatter.get("url"),
                source=frontmatter.get("session_id"),
                chunk_number=frontmatter.get("chunk_number"),
                title=frontmatter.get("title"),
                summary=frontmatter.get("summary"),
                embedding=frontmatter.get("embedding"),
                content=chunk,
            )

            return res

        except amme.MafiaError as e:
            print(e)
            return False

    def compare_self_to_disk(self, md_path):
        if not os.path.exists(md_path):
            return False

        try:
            md_chunk = self.from_md_file(md_path=md_path)

        except PC_PathNotExist as e:
            print(e)
            return self

        if not md_chunk:
            return self

        if md_chunk.content == self.content:
            self.title = self.title or md_chunk.title
            self.summary = self.summary or md_chunk.summary
            self.embedding = self.embedding or md_chunk.embedding
            self.metadata = md_chunk.metadata
            self.error_logs = md_chunk.error_logs

        return self

    async def get_title_and_summary(
        self,
        is_replace_llm_metadata: bool = False,
        async_client: AsyncOpenAI = None,
        model="gpt-4o-mini-2024-07-18",
        debug_prn: bool = False,
        return_raw: bool = False,
    ) -> dict:

        async_client = async_client or openai_routes.default_async_openai_client

        if not is_replace_llm_metadata and self.title and self.summary:
            if debug_prn:
                print(f"🛢️ {self.url} title and summary already exists")
            return self

        system_prompt = crawler_prompts.prompt_extract_title_and_summary

        messages = [
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": f"URL: {self.url}\n\nContent:\n{self.content[:1000]}...",
            },  # Send first 1000 chars for context
        ]
        try:
            res = await openai_routes.generate_openai_chat(
                messages=messages,
                async_client=async_client,
                model=model,
                response_format={"type": "json_object"},
                return_raw=return_raw,
            )

            if return_raw:
                return res

            self.title = res.response.get("title")
            self.summary = res.response.get("summary")

            return res

        except amme.MafiaError as e:
            message = f"Error getting title and summary: {e}"

            print(message)
            self.error_logs.append(message)

            return False

    async def get_embedding(
        self,
        is_replace_llm_metadata: bool = False,
        async_client: AsyncOpenAI = None,
        model="text-embedding-3-small",
        return_raw: bool = False,
        debug_prn: bool = False,
    ) -> List[float]:

        if not is_replace_llm_metadata and self.embedding:

            if debug_prn:
                print(f"🛢️  {self.url} embedding already retrieved")

            return self

        try:
            res = await openai_routes.generate_openai_embedding(
                text=self.content,
                async_client=async_client,
                model=model,
                return_raw=return_raw,
                debug_prn=debug_prn,
            )

            if return_raw:
                return res

            self.embedding = res
            return res

        except amme.MafiaError as e:
            message = f"Error creating embedding: {e}"

            self.error_logs.append(message)

            return False

    async def generate_metadata(
        self,
        is_replace_llm_metadata: bool = False,
        async_text_client: AsyncOpenAI = None,
        async_embedding_model: AsyncOpenAI = None,
        text_model="gpt-4o-mini-2024-07-18",
        embedding_model="text-embedding-3-small",
        debug_prn: bool = False,
        output_path: str = None,
    ):
        await self.get_title_and_summary(
            is_replace_llm_metadata=is_replace_llm_metadata,
            async_client=async_text_client,
            model=text_model,
            debug_prn=debug_prn,
        )
        await self.get_embedding(
            is_replace_llm_metadata=is_replace_llm_metadata,
            async_client=async_embedding_model,
            model=embedding_model,
            debug_prn=debug_prn,
        )

        if output_path:
            storage_routes.save_chunk_to_disk(output_path=output_path,
                                              data = self.to_json())

        return self

    def to_json(self):
        return {
            "url": self.url,
            "source": self.source,
            "chunk_number": self.chunk_number,
            "title": self.title or "No Title",
            "summary": self.summary or "No Summary",
            "content": self.content,
            "metadata": self.metadata.to_json(),
            "embedding": self.embedding or [0] * 1536,
        }


async def process_chunk(
    url,
    chunk,
    chunk_number,
    source,
    async_supabase_client,
    database_table_name,
    export_folder,
    is_replace_llm_metadata: bool = False,
    debug_prn: bool = False,
):
    if debug_prn:
        print(f"🎬 starting {url} - {chunk_number}")

    chunk_path = (
        f"{export_folder}/chunks/{amcv.convert_url_file_name(url)}/{chunk_number}.md"
    )

    chunk = pc.Crawler_ProcessedChunk.from_chunk(
        chunk=chunk,
        chunk_number=chunk_number,
        url=url,
        source=source,
        output_path=chunk_path,
    )

    # try:
    await chunk.generate_metadata(
        output_path=chunk_path,
        is_replace_llm_metadata=is_replace_llm_metadata,
        debug_prn=debug_prn,
    )

    data = chunk.to_json()
    data.pop("source")

    await storage_routes.store_data_in_supabase_table(
        async_supabase_client=async_supabase_client,
        table_name=database_table_name,
        data=data,
    )

    if debug_prn:
        print(f"successfully processed {url}-{chunk_number}")

    return chunk

    # except Exception as e:
    #     print(
    #         utils.generate_error_message(
    #             f"💀 process_chunk - {url} - {chunk_number} -{e}", exception=e
    #         )
    #     )

# %% ../../nbs/implementations/scrape_urls.ipynb 5
async def read_url(
    url,
    source,
    browser_config: crawler_routes.BrowserConfig,
    doc_path,
    crawler_config: crawler_routes.CrawlerRunConfig = None,
    debug_prn: bool = False,
):
    if os.path.exists(doc_path):
        content, _ = amfi.read_md_from_disk(doc_path)

        if debug_prn:
            print(f"🛢️  {url} - scraping not required, file retrieved from - {doc_path}")

        return content

    storage_fn = partial(storage_routes.save_chunk_to_disk,
        output_path=doc_path,
        )

    res = await crawler_routes.scrape_url(
        url=url,
        session_id=source,
        browser_config=browser_config,
        crawler_config=crawler_config,
        storage_fn = storage_fn
    )
    if debug_prn:
        print(f"🛢️  {url} - page scraped to {doc_path}")

    return res.markdown

# %% ../../nbs/implementations/scrape_urls.ipynb 6
async def process_url(
    url: str,
    source: str,
    export_folder: str,
    database_table_name: str,
    async_supabase_client=None,
    debug_prn: bool = False,
    browser_config: crawler_routes.BrowserConfig = None,
    crawler_config: crawler_routes.CrawlerRunConfig = None,
    is_replace_llm_metadata: bool = False,
    max_conccurent_requests=5,
):
    """process a document and store chunks in parallel"""

    browser_config = browser_config or crawler_routes.default_browser_config
    async_supabase_client = async_supabase_client or storage_routes.async_supabase_client

    doc_path = f"{export_folder}/{amcv.convert_url_file_name(url)}.md"

    ## scrape url and save results to doc_path
    try:
        if debug_prn:
            print(f"starting crawl - {url}")

        markdown = await read_url(
            url=url,
            source=source,
            browser_config=browser_config,
            doc_path=doc_path,
            debug_prn=debug_prn,
            crawler_config=crawler_config,
        )

    except Exception as e:
        print(f"⛔  {url} - error while read_url - {e}")
        return False

    if debug_prn:
        print(f"☀️  successfully crawled: {url}")

    chunks = amcn.chunk_text(markdown)

    if debug_prn:
        print(f"☀️  : {len(chunks)} to process {url}")

    res = await amce.gather_with_concurrency(
        *[
            process_chunk(
                url=url,
                chunk=chunk,
                chunk_number=idx,
                source=source,
                async_supabase_client=async_supabase_client,
                database_table_name=database_table_name,
                export_folder=export_folder,
                debug_prn=debug_prn,
                is_replace_llm_metadata=is_replace_llm_metadata,
            )
            for idx, chunk in enumerate(chunks)
        ],
        n=max_conccurent_requests,
    )

    if debug_prn:
        print(f"☀️  done processing url {url}")

    return res

# %% ../../nbs/implementations/scrape_urls.ipynb 7
async def process_rgd(
    rgd,
    source : str, 
    export_folder: str,
    database_table_name: str = "site_pages",
    supabase_client=None,
    debug_prn: bool = False,
    is_replace_llm_metadata: bool = False,
    max_conccurent_requests=5,
):

    supabase_client = supabase_client or storage_routes.async_supabase_client

    ## scrape url and save results to doc_path
    if debug_prn:
        print(f"processing - {rgd.url}")

    chunks = amcn.chunk_text(rgd.markdown)

    if debug_prn:
        print(f"☀️  : {len(chunks)} to process {rgd.url}")

    res = await amce.gather_with_concurrency(
        *[
            process_chunk(
                url=rgd.url,
                chunk=chunk,
                chunk_number=idx,
                source=source,
                async_supabase_client=supabase_client,
                database_table_name=database_table_name,
                export_folder=export_folder,
                debug_prn=debug_prn,
                is_replace_llm_metadata=is_replace_llm_metadata,
            )
            for idx, chunk in enumerate(chunks)
        ],
        n=max_conccurent_requests,
    )

    if debug_prn:
        print(f"☀️  done processing url {rgd.url}")

    return res

# %% ../../nbs/implementations/scrape_urls.ipynb 8
async def process_urls(
    urls: List[str | None],
    source: str,
    export_folder: str = "./export",
    database_table_name: str = "site_pages",
    max_conccurent_requests: int = 5,
    debug_prn: bool = False,
    browser_config : crawler_routes.BrowserConfig =None,
    crawler_config : crawler_routes.CrawlerRunConfig = None,
    is_replace_llm_metadata: bool = False,
):
    if not urls:
        print("No URLs found to crawl")
        return

    urls_path = f"./export/urls/{source}.txt"

    amfi.upsert_folder(urls_path)

    with open(urls_path, "w+", encoding="utf-8") as f:
        f.write("\n".join(urls))

    res = await amce.gather_with_concurrency(
        *[
            process_url(
                url=url,
                source=source,
                debug_prn=debug_prn,
                browser_config=browser_config,
                export_folder=export_folder,
                database_table_name=database_table_name,
                is_replace_llm_metadata=is_replace_llm_metadata,
                crawler_config=crawler_config
            )
            for url in urls
        ],
        n=max_conccurent_requests,
    )

    print("done")
    return res
